---
title: \vspace{-2.0cm} \textbf{Web Traffic Forecasting}
subtitle: Time Series Analysis
author:
- \small \textbf{Shree Karimkolikuzhiyil}; Programming; Statistics, M.S. (Distance); [shreejesh\@tamu.edu](mailto:shreejesh@tamu.edu){.email}
- \small \textbf{Jingcheng Xia}; Computations; Computer Science, B.S. (On-Campus); [sixtyfour64\@tamu.edu](mailto:sixtyfour64@tamu.edu){.email}
- \small \textbf{Jackson Smith}; Analysis; Statistics, M.S. (Distance);  [jackson.t.smith\@tamu.edu](mailto:jackson.t.smith@tamu.edu){.email}
- \small \textbf{Samuel Burge}; Writing; Statistics, M.S. (Distance);  [samuelburge\@tamu.edu](mailto:samuelburge@tamu.edu){.email}
- \small \textbf{Max Kutschinski}; Theory; Statistics, M.S. (Distance); [mwk556\@tamu.edu](mailto:mwk556@tamu.edu){.email}
output:
  pdf_document:
    fig_width: 17
    fig_height: 6
fontsize: 11pt
---
\vspace{-1.5cm}  

---
## Introduction and Motivation

<!-- Introduce Your data: Explain the context and background story of your data -->
The objective of this analysis is to forecast daily unique visitors to an academic website containing lecture notes and supplemental material related to statistics. Predicting website traffic allows IT departments to manage project throughput and prioritize maintenance and enhancements to website functionality and effectively allocate web server resources. Web traffic is also a key indicator of customer growth and expansion, as well as sustaining recurring customers and ingrained growth. The details provided by web traffic throughput reports contain many metrics, including page loads, returning visitors, and unique visits, each of which conveys a different picture and set of information for an organization. As well, having a picture of expected throughput and confirming (or denying) expectations with reality allows a business to understand unexpected growth and/or unexpected decay in business development.

The data contains five years of daily time series data of user visits. There are four features in the data set, which include daily counts for the number of page loads, first-time visitors, returning visitors, and unique visitors.^[A visit is defined as a stream of hits on one or more pages on the site on a given day by the same user within a 6-hour window, identified by the IP address of the specific device. Returning visitors are identified through allowed cookies on a user's device, and the total number of returning and first-time visitors is, by definition, the number of unique visitors.] An initial plot of the data shows strong seasonality and volatility, but doesn't appear to have any discernible trend or cyclical behavior. An explanation for this could be due to the nature of the website. Students would likely be the largest share of users for a website of this nature, and the seasonality seems associated with the academic calendar typically seen at academic institutions.  
\  

<!-- Plot the data and summarize your preliminary findings: trend, cycle, volatility, etc. -->
```{r include=F, echo=F}
# load necessary packages
library(readr)
library(astsa)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(ggfortify) # imports autoplot() function for plotting acf plots more easily
library(tidyverse)
library(forecast) 
library(Hmisc) # Lowess smoothing
library(tseries) #arma
library(knitr) #table
library(showtext) # package to use the Latex font in plots
font_add(family = "cm", regular = "computermodern.ttf")
showtext_auto()

webData = read_csv("WebTraffic.csv",
                   skip = 1,
                   col_names = c("Day",
                                 "DayOfWeek",
                                 "Date",
                                 "PageLoads",
                                 "UniqueVisits",
                                 "FirstTimeVisits",
                                 "ReturningVisits"),
                   col_types = cols("Day" = "f",
                                    "Date" = col_date("%m/%d/%Y")))

tsData = ts(webData, start = decimal_date(as.Date("2014-09-14")), frequency = 365) # time series object
```

```{r include=F, echo=F, eval = F}
colnames(webData)
anyNA(webData)
dim(webData)
str(webData)
summary(webData)
```  


```{r Fig1, echo=F}

# Set the theme for the chart
theme_set(theme_bw() + theme(text = element_text(family = 'cm', size = 30)))

# Plot the daily count of unique page visits
ggplot(webData, aes(Date, UniqueVisits)) +
  geom_line(color = '#437cb5') +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(title = "Unique Page Visits Over Time", caption = "Figure 1") + 
  ylab("Unique Visits")
```

## Stationarity

Stationarity is a common assumption underlying many time series procedures. As such, it is important to assess the level of stationarity prior to modeling and make the appropriate adjustments if necessary.

A stationary time series is one whose properties do not depend on the time at which the series is observed. More specifically,

(i) *the mean value function* $\mu_t=E(x_t)$ *is constant and does not depend on time t*

(ii) *the autocovariance function* $\gamma(s,t)=cov(x_s,x_t)=E[(x_s-\mu_s)(x_t-\mu_t)]$ *depends on times s and t only though their lagged difference.* 


The strong seasonality that is apparent in Figure 1 is indicative of non-stationarity, since seasonality will affect the value of the time series at different times. Seasonality is defined as a recurring pattern at a fixed and known frequency based on the time of the year, week, or day. 

Figures 2 and 3 aim to identify the types of seasonality present in the data. Figure 2 plots a subset of the first several weeks and indicates that there exists weekly seasonality, whereas Figure 3 uses locally weighted scatterplot smoothers (Lowess) to emphasize the inherent yearly seasonality.  
\newline

```{r Fig2, echo= F, message=F}
# Plot the daily count of unique page visits

ggplot(webData[1:90, ], aes(Date, UniqueVisits)) +
  geom_line(color = '#437cb5') +
  geom_point(color = '#437cb5') +
  scale_x_date(date_breaks = "1 week",
               date_minor_breaks = "1 day",
               date_labels = '%W') +
  labs(title = "Weekly Seasonality", caption = 'Figure 2: Sample of weekly page visits') + 
  ylab("Unique Visits") +
  xlab("Date")+
  theme(axis.text.x = element_text(size = 20, angle = 90),
        panel.grid.major.x = element_line(color = 'darkgrey'),
        panel.grid.minor.x = element_line(color = 'lightgrey', linetype = 'dashed'))
```



```{r Fig3, echo= F, message=F}
autoplot(tsData[,"UniqueVisits"], color = '#437cb5')+
  scale_x_continuous(breaks = seq(from = 2014, to =  2021, by = 1))+
  stat_plsmo(color="black", span= 0.05)+  
  labs(title = "Yearly Seasonality", caption = "Figure 3: Smoothing via Lowess") + 
  ylab("Unique Visits")+
  xlab("Date")
```

\newpage


A popular approach in addressing non-stationarity due to seasonalities is to eliminate these effects via seasonal differencing. The seasonal difference of a time series is the series of changes from one season to the next, which is defined as folllows:

\begin{equation}
  \triangledown x_t=x_t-x_{t-m}
\end{equation}

Hence, yearly and weekly seasonalities will be handled by computing the lag 365 and lag 7 seasonal difference, respectively.

\begin{equation}
  \triangledown x_t^*=(x_t-x_{t-365})-x_{t-7}
\end{equation}

Time plots of our series at different levels of differencing are displayed in Figure 4. The differenced series appears to be stationary with constant mean and variance.
\newline

```{r Fig4, echo=F, message=F, fig.height=5}
# seasonal differences
diff0 = tsData[,"UniqueVisits"]
diff1 = diff(tsData[,"UniqueVisits"],lag=365) # eliminate yearly seasonality
diff2=  diff(diff1,lag=7) # eliminate weekly seasonality

# Time plots
autoplot(diff0)+
  geom_line(color = '#437cb5') +
  labs(title = "Time Plots of Differenced Series", subtitle = "No Difference")+
  theme(plot.title = element_text(hjust=0.5))+
  ylab("Unique Visits")
autoplot(diff1)+
  geom_line(color = '#437cb5')+
  labs(subtitle = "Lag 365 Difference")
autoplot(diff2)+
  geom_line(color = '#437cb5') +
  labs(subtitle = "Lag 7 Difference of Previous Difference", caption = 'Figure 4: Time plots of differenced series')
```
\newpage

The ACF and PACF of the differenced series $\triangledown x_t^*$ are displayed in Figure 5. Neither the ACF nor the PACF seems to cut off after a certain lag, which would be indicative of an AR or MA process. Rather, both of them appear to tail off over time. 
\newline

```{r Fig5, echo=F, message=F, fig.height=5}
# ACF and PACF plots
ggAcf(diff2, color='#437cb5', lag.max = 50)+
  labs(title = "ACF and PACF After Differencing")+
  scale_x_continuous(breaks=seq(0,100,7))+
  theme(plot.title = element_text(hjust=0.5))
ggPacf(diff2, color='#437cb5', lag.max = 50)+
  scale_x_continuous(breaks=seq(0,100,7))+
  labs(title="", caption = 'Figure 5: ACF and PACF')
```

<!--
Problems with this approach:
- high number of lags is generally not well received by models
- moving holdiday effects (date changes per year)
- leap years
-->

## Modeling

Both ACF and PACF show a slow decay indicating that an ARMA(2,2) model could be appropriate for the series. An auto regressive moving average of order p and q (ARMA(p,q)) model is defined in Eq(3):

\begin{equation}
   x_t = \alpha + \phi_1x_{t-1}+\cdot\cdot\cdot+\phi_px_{t-p}+w_t+\theta_1\omega_{t-1}+\cdot\cdot\cdot+\theta_p\omega_{t-q}\\
\end{equation}
\begin{center}where $\phi_p\neq0, \theta_p\neq0,\sigma_w^2>0$, and the model is causal and invertible.\end{center}

```{r arma, echo=F}
#fit model
#arma22 = arma(diff2, order=c(2,2), include.intercept = FALSE)
#new
sarima1 = sarima(diff2, 1,0,2,3,0,3,S=7)
summary(sarima1)
```

The parameters of the ARMA(2,2) model are estimated via conditional least squares and are displayed in Eq(4).

\begin{equation}
  \hat{x}_t = 0.44x_{t-1}+0.13x_{t-2}+w_t+0.05\omega_{t-1}-0.09\omega_{t-2}
\end{equation}
\newpage
Figure 6 displays of plot of the residuals of the fitted ARMA(2,2) model. The residuals seem to behave like white noise, being centered around 0 with constant mean and variance. 
\newline

```{r Fig6, echo=FALSE, warning=F, eval=F}
res = residuals(arma22)
autoplot(res)+
  geom_line(color = '#437cb5') +
  labs(title = "Residual Plot", caption = 'Figure 6: Residuals of ARMA(2,2) model')+
  ylab("Residual")
```


Table 1 contains the roots of the model's polynomials. Since they appear to be different from each other by a reasonable margin, we can conclude that there is no parameter redundancy in the model.

```{r roots, echo = F, eval=F}
# checking for roots
AR = c(1, -arma22$coef[[1]], -arma22$coef[[2]])
MA = c(1, arma22$coef[[3]], arma22$coef[[4]])
polyAR = polyroot(AR)
polyMA = polyroot(MA)
roots = round(data.frame(polyAR,polyMA),3)

kable(roots, col.names = c("AR","MA"), align = c("c","c"), caption = "Roots of polynomials")
```

The fitted values of the ARIMA(2,2) model are plotted against the actual values of the differenced series $\triangledown x_t^*$ in Figure 7. 
\newline

```{r Fig7, echo=F, message=FALSE, results='hide', eval=F}
fit = Arima(diff2, order=c(2,0,2))
df2 = cbind(diff2,fit$fitted)
autoplot(df2)+
  scale_color_manual(labels= c("actual","fitted"),values=c("black", "#437cb5"))+
  labs(title = "Fitted vs Actual Values", caption = 'Figure 7: Fitted vs actual values using differenced series')+
  ylab("Difference")+
  theme(legend.position=c(0.94,0.9),
        legend.title = element_blank(),
        legend.margin = margin(t = 0, unit='cm'))+
  guides(color = guide_legend(override.aes = list(size = 3) ) )


############ end v2
```


```{r include=F, eval=F, echo=F}
# Predictions on actual data

pred = rep(0,length(diff0))
for (t in 1:length(fit$fitted)){
  pred[t+365+7]= fit$fitted[t]+diff1[t]+diff0[t]
}

tsPred = ts(pred,start = decimal_date(as.Date("2014-09-14")), frequency = 365)
actualPred = cbind(diff0,tsPred)
autoplot(actualPred)+
  scale_color_manual(values=c("black", "#437cb5"))

```

```{r, eval=F}

m=dshw(subset(tsData[,5],end=length(tsData[,5])-7),h=7)


auto.arima(tsData[,5], seasonal=T)
```


When there are long seasonal periods, a dynamic regression with Fourier terms is often better than other models we have considered in this book.
Seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data.

```{r, eval=F}
fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
    seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit,
      xreg=fourier(cafe04, K=i, h=24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)

```


```{r, eval=F}

m=mstl(tsData[,5])
plot(m)

# train test split
UniqueVisits = tsData[,5]
training <- subset(UniqueVisits, end=length(UniqueVisits)-365)
test <- subset(UniqueVisits, start=length(UniqueVisits)-364)

# baseline

web.train <- Arima(training, order=c(2,1,1),
  seasonal=c(0,1,2), lambda=0)
cafe.train %>%
  forecast(h=60) %>%
  autoplot() + autolayer(test)

snaive(test,h=7)
```


```{r, eval = F}
tsData[,5] %>%  stlf() %>%
  autoplot() + xlab("Week")



fit <- auto.arima(tsData[,5], seasonal=FALSE,
         xreg=fourier(tsData[,5], K=c(3,3))
fit %>%
  forecast(xreg=fourier(tsData[,5], K=c(10,10))) %>%
  autoplot() +
    ylab("Call volume") + xlab("Weeks")

```