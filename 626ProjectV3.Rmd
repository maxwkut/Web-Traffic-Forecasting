---
title: \vspace{-2.0cm} \textbf{Web Traffic Forecasting}
subtitle: Time Series Analysis
author:
- \small \textbf{Shree Karimkolikuzhiyil}; Programming; Statistics, M.S. (Distance); [shreejesh\@tamu.edu](mailto:shreejesh@tamu.edu){.email}
- \small \textbf{Jingcheng Xia}; Computations; Computer Science, B.S. (On-Campus); [sixtyfour64\@tamu.edu](mailto:sixtyfour64@tamu.edu){.email}
- \small \textbf{Jackson Smith}; Analysis; Statistics, M.S. (Distance);  [jackson.t.smith\@tamu.edu](mailto:jackson.t.smith@tamu.edu){.email}
- \small \textbf{Samuel Burge}; Writing; Statistics, M.S. (Distance);  [samuelburge\@tamu.edu](mailto:samuelburge@tamu.edu){.email}
- \small \textbf{Max Kutschinski}; Theory; Statistics, M.S. (Distance); [mwk556\@tamu.edu](mailto:mwk556@tamu.edu){.email}
output:
  pdf_document:
    fig_width: 17
    fig_height: 6
    citation_package: natbib
fontsize: 11pt
bibliography: citations.bib
---
\vspace{-1.5cm}  

---
# Introduction and Motivation

<!-- Introduce Your data: Explain the context and background story of your data -->
The objective of this analysis is to forecast daily unique visitors to an academic website over a 30-day horizon. Predicting website traffic allows IT departments to manage project throughput and prioritize maintenance and enhancements to website functionality and effectively allocate web server resources. Web traffic is also a key indicator of customer growth and expansion, as well as sustaining recurring customers and ingrained growth. The details provided by web traffic throughput reports contain many metrics, including page loads, returning visitors, and unique visits, each of which conveys a different picture and set of information for an organization. As well, having a picture of expected throughput and confirming (or denying) expectations with reality allows a business to understand unexpected growth and/or unexpected decay in business development.

The data contains five years of daily time series data of user visits. There are four features in the data set, which include daily counts for the number of page loads, first-time visitors, returning visitors, and unique visitors.^[A visit is defined as a stream of hits on one or more pages on the site on a given day by the same user within a 6-hour window, identified by the IP address of the specific device. Returning visitors are identified through allowed cookies on a user's device, and the total number of returning and first-time visitors is, by definition, the number of unique visitors.] An initial plot of the data shows strong seasonality and volatility, but doesn't appear to have any discernible trend or cyclical behavior. An explanation for this could be due to the nature of the website. Students would likely be the largest share of users for a website of this nature, and the seasonality seems associated with the academic calendar typically seen at academic institutions.  
\  

<!-- Plot the data and summarize your preliminary findings: trend, cycle, volatility, etc. -->
```{r include=F, echo=F}
# load necessary packages
library(readr)
library(astsa)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(ggfortify) # imports autoplot() function for plotting ACF plots more easily
library(tidyverse)
library(forecast)
library(fable)   # New version of the forecast package
library(tsibble) # Temporal tbl objects for use in the tidyverse
library(feasts)  # Necessary for the fable package
library(fable.prophet) # package to use prophet models with fable workflow
library(Hmisc)   # Lowess smoothing
library(tseries) # ARMA
library(knitr)   # Table
library(showtext) # package to use the Latex font in plots
font_add(family = "cm", regular = "computermodern.ttf")
showtext_auto()

webData = read_csv("WebTraffic.csv",
                   skip = 1,
                   col_names = c("Day",
                                 "DayOfWeek",
                                 "Date",
                                 "PageLoads",
                                 "UniqueVisits",
                                 "FirstTimeVisits",
                                 "ReturningVisits"),
                   col_types = cols("Day" = "f",
                                    "Date" = col_date("%m/%d/%Y")))

tsData = ts(webData, start = decimal_date(as.Date("2014-09-14")), frequency = 365) # time series object
```

```{r include=F, echo=F, eval = F}
colnames(webData)
anyNA(webData)
dim(webData)
str(webData)
summary(webData)
```  


```{r Fig1, echo=F}
# Set the theme for the chart
theme_set(theme_bw() + theme(text = element_text(family = 'cm', size = 30)))

# Plot the daily count of unique page visits
ggplot(webData, aes(Date, UniqueVisits)) +
  geom_line(color = '#437cb5') +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(title = "Unique Page Visits Over Time", caption = "Figure 1") + 
  ylab("Unique Visits")
```

# Modeling

## SARIMA

Stationarity is a common assumption underlying many time series procedures. As such, it is important to assess the level of stationarity prior to modeling and make the appropriate adjustments if necessary.

@shumway describe a stationary time series as one whose properties do not depend on the time at which the series is observed. More specifically,

(i) *the mean value function* $\mu_t=E(x_t)$ *is constant and does not depend on time t*

(ii) *the autocovariance function* $\gamma(s,t)=cov(x_s,x_t)=E[(x_s-\mu_s)(x_t-\mu_t)]$ *depends on times s and t only though their lagged difference.* 


The strong seasonality that is apparent in Figure 1 is indicative of non-stationarity, since seasonality will affect the value of the time series at different times. Seasonality is defined as a recurring pattern at a fixed and known frequency based on the time of the year, week, or day. 

Figures 2 and 3 aim to identify the types of seasonality present in the data. Figure 2 plots a subset of the first several weeks and indicates that there exists weekly seasonality, whereas Figure 3 uses locally weighted scatterplot smoothers (Lowess) to emphasize the inherent yearly seasonality.  
\newline

```{r Fig2, echo= F, message=F, fig.height =5}
# Plot the daily count of unique page visits
ggplot(webData[1:90, ], aes(Date, UniqueVisits)) +
  geom_line(color = '#437cb5') +
  geom_point(color = '#437cb5') +
  scale_x_date(date_breaks = "1 week",
               date_minor_breaks = "1 day",
               date_labels = '%W') +
  labs(title = "Weekly Seasonality", caption = 'Figure 2: Sample of weekly page visits') + 
  ylab("Unique Visits") +
  xlab("Date")+
  theme(axis.text.x = element_text(size = 20, angle = 90),
        panel.grid.major.x = element_line(color = 'darkgrey'),
        panel.grid.minor.x = element_line(color = 'lightgrey', linetype = 'dashed'))
```



```{r Fig3, echo= F, message=F, fig.height=5}
autoplot(tsData[,"UniqueVisits"], color = '#437cb5')+
  scale_x_continuous(breaks = seq(from = 2014, to =  2021, by = 1))+
  stat_plsmo(color="black", span= 0.05)+  
  labs(title = "Yearly Seasonality", caption = "Figure 3: Smoothing via Lowess") + 
  ylab("Unique Visits")+
  xlab("Date")
```

\newpage


A popular approach in addressing non-stationarity due to seasonality is to eliminate these effects via seasonal differencing. The seasonal difference of a time series is the series of changes from one season to the next, which is defined as follows:

\begin{equation}
  \triangledown x_t=x_t-x_{t-m}
\end{equation}

Hence, yearly and weekly seasonality will be handled by computing the lag 365 and lag 7 seasonal difference, respectively.

\begin{equation}
  \triangledown x_t^*=(x_t-x_{t-365})-x_{t-7}
\end{equation}

Time plots of our series at different levels of differencing are displayed in Figure 4. The differenced series appears to be stationary with constant mean and variance.
\newline

```{r Fig4, echo=F, message=F, fig.height=5}
# Differencing to induce stationarity
web = webData %>%
  select(Date, UniqueVisits) %>%
  mutate(weekly_lag = lag(UniqueVisits, n = 7),                     # x_{t-7}
         annual_lag = lag(UniqueVisits, n = 365),                   # x_{t-365}
         total_lag = lag(lag(UniqueVisits, n = 365), n = 7),        # x_{t-365-7}
       
         weekly_diff = UniqueVisits - lag(UniqueVisits, n = 7),     # Weekly difference
         annual_diff = UniqueVisits - lag(UniqueVisits, n = 365),   # Annual difference
         diff = annual_diff - lag(annual_diff, n = 7)) %>%          # Eliminates annual AND weekly seasonality
  filter(!is.na(diff)) %>%
  as_tsibble(index = Date)

train = webData %>%
  select(Date, UniqueVisits) %>%
  mutate(weekly_lag = lag(UniqueVisits, n = 7),                     # x_{t-7}
         annual_lag = lag(UniqueVisits, n = 365),                   # x_{t-365}
         total_lag = lag(lag(UniqueVisits, n = 365), n = 7),        # x_{t-365-7}
       
         weekly_diff = UniqueVisits - lag(UniqueVisits, n = 7),     # Weekly difference
         annual_diff = UniqueVisits - lag(UniqueVisits, n = 365),   # Annual difference
         diff = annual_diff - lag(annual_diff, n = 7)) %>%          # Eliminates annual AND weekly seasonality
  filter(!is.na(diff), Date < '2019-09-14') %>%
  as_tsibble(index = Date)

test = webData %>%
  select(Date, UniqueVisits) %>%
  mutate(weekly_lag = lag(UniqueVisits, n = 7),                     # x_{t-7}
         annual_lag = lag(UniqueVisits, n = 365),                   # x_{t-365}
         total_lag = lag(lag(UniqueVisits, n = 365), n = 7),        # x_{t-365-7}
         
         weekly_diff = UniqueVisits - lag(UniqueVisits, n = 7),     # Weekly difference
         annual_diff = UniqueVisits - lag(UniqueVisits, n = 365),   # Annual difference
         diff = annual_diff - lag(annual_diff, n = 7)) %>%          # Eliminate annual AND weekly seasonality
  filter(!is.na(diff), Date >= '2019-09-14') %>%
  as_tsibble(index = Date)

# See the relative size of each set in comparison to total available data
#nrow(train) / nrow(webData) # About 85% of the available data
#nrow(test) / nrow(webData)  # About 15% of the available data

# Time plots
ggplot(train, aes(Date, UniqueVisits)) +
  geom_line(color = '#437cb5') +
  labs(title = "Time Plots of Differenced Series", subtitle = "No Difference") +
  theme(plot.title = element_text(hjust=0.5)) +
  ylab("Unique Visits")

ggplot(train, aes(Date, annual_diff)) +
  geom_line(color = '#437cb5') +
  labs(subtitle = "Lag 365 Difference")

ggplot(train, aes(Date, diff)) +
  geom_line(color = '#437cb5') +
  labs(subtitle = "Lag 7 Difference of Previous Difference", caption = 'Figure 4: Time plots of differenced series')
```
\newpage

The ACF and PACF of the differenced series $\triangledown x_t^*$ are displayed in Figure 5. Neither the ACF nor the PACF seems to cut off after a certain lag, which would be indicative of an AR or MA process. Rather, both of them appear to tail off over time. 
\newline

```{r Fig5, echo=F, message=F, fig.height=5}
# ACF and PACF plots
ggAcf(train$diff, color='#437cb5', lag.max = 365) +
  labs(title = "ACF and PACF After Differencing") +
  theme(plot.title = element_text(hjust=0.5))

ggPacf(train$diff, color='#437cb5', lag.max = 365) +
  labs(title="", caption = 'Figure 5: ACF and PACF')
```

<!--
Problems with this approach:
- high number of lags is generally not well received by models
- moving holdiday effects (date changes per year)
- leap years
-->

Both ACF and PACF show a slow decay making it difficult to determine specific orders for the family of ARMA(p,q) models defined in Eq(3) as:

\begin{equation}
   x_t = \alpha + \phi_1x_{t-1}+\cdot\cdot\cdot+\phi_px_{t-p}+w_t+\theta_1\omega_{t-1}+\cdot\cdot\cdot+\theta_p\omega_{t-q}\\
\end{equation}
\begin{center}where $\phi_p\neq0, \theta_p\neq0,\sigma_w^2>0$, and the model is causal and invertible.\end{center}  

We opted to fit a range of ARMA(p,q) models with small orders, with the final model selected based the accuracy of the model forecasts test or hold-out set since our primary goal for this analysis is forecasting (not necessarily inferences or hypothesis testing). The model selection criterion for the fitted ARMA models is shown in the table below. 

```{r arma, echo=F}
# Fit various models to the training set
models = train %>%
  model('Seasonal Naive' = fable::SNAIVE(diff ~ lag(365)),              # Baseline for comparison (seasonal naive model)
        'ARMA(2,2)' = ARIMA(diff ~ 0 + pdq(2, 0, 2) + PDQ(0, 0, 0)),    # Fit based on our assessment of the ACF/PACF plots
        'ARMA(1,1)' = ARIMA(diff ~ 0 + pdq(2,0,2)),
        'ARMA(1,2)' = ARIMA(diff ~ 0 + pdq(1,0,2)),
        'ARMA(2,1)' = ARIMA(diff ~ 0 + pdq(2,0,1)))
  
# Retrieves the parameter estimates and hypothesis tests (p-values) for the models
models %>%
  glance() %>%
  select(-ar_roots, -ma_roots) %>%
  filter(.model != 'Seasonal Naive') %>%
  arrange(AIC) %>%
  kable(col.names = c('Model',expression(sigma^2),'Log-Likelihood','AIC','AICc','BIC'))
```

The parameters of the ARMA(1,2) model are estimated via conditional least squares and are displayed in Eq(4).

\begin{equation}
  \hat{x}_t = 0.44x_{t-1}+0.13x_{t-2}+w_t+0.05\omega_{t-1}-0.09\omega_{t-2}
\end{equation}  

Figure 6 displays of plot of the residuals of the fitted ARMA(1,2) model. Initially, the residuals seem to behave like white noise, being centered around zero with a constant variance. However, further analysis of autocorrelation plot and formal testing using the Box-Ljung test indicate that the innovations are not uncorrelated (i.e., they are not white noise). The autocorrelation in the innovations does appear small for most lags, given how similar the various models are this is likely the best fit we can obtain from the ARMA(p,q) modelling procedure.
\newline

```{r Fig6, echo=FALSE, warning=F}
# Residual diagnostics for the ARMA(2,2) model
models %>%
  select('ARMA(1,2)') %>%
  gg_tsresiduals() +
  ggtitle('Residual/Innovation Diagnostic Plots')

# Compute the test statistic(s) for the Box-Ljung test and plot them
box_ljung_table = tibble(h = seq(5, 20),
                         lb_stat = rep(0, 16),
                         lb_pvalue = rep(0, 16))

for(i in (1:nrow(box_ljung_table))) {
  calc = models %>%
    select('ARMA(1,2)') %>%
    augment() %>%
    features(.resid, ljung_box, lag = as.numeric(box_ljung_table[i, 'h']))
  
  box_ljung_table[i,'lb_stat'] = calc[1,'lb_stat']
  box_ljung_table[i,'lb_pvalue'] = calc[1,'lb_pvalue']
}

ggplot(box_ljung_table, aes(x = h, y = lb_pvalue)) +
  geom_point(pch = 1) +
  geom_hline(yintercept = 0.05, color = 'blue', linetype = 'dashed') +
  coord_cartesian(ylim = c(0,1)) +
  labs(y = 'p-values', x = 'Lags (H)', title = 'P-values for the Box-Ljung statistic')
```


Table 1 contains the model's polynomial roots. Since they appear to be different from each other by a reasonable margin, we can conclude that there is no parameter redundancy in the model.

```{r roots, echo = F}
# checking for roots
roots = models %>%
  select('ARMA(1,2)') %>%
  glance() %>%
  select(ar_roots, ma_roots)

models %>%
  select('ARMA(1,2)') %>%
  gg_arma()

kable(roots, col.names = c("AR","MA"), align = c("c","c"), caption = "Roots of polynomials", digits = 2)
```

The fitted values of the ARMA(1,2) model are plotted against the actual values of the seasonally differenced series $\triangledown x_t^*$ (Figure 7) and the. 
\newline

```{r Fig7, echo=F, message=FALSE, results='hide', warning=FALSE}
# See the detailed summary for the model specifications
arima_mdl_details = models %>%
  report() %>%
  filter(.model == "ARMA(1,2)")

# Plot the fitted versus actual values
fit_data = models %>%
  augment() %>%
  filter(.model == "ARMA(1,2)") %>%
  select(Date, actual = diff, fit = .fitted)
  
ggplot(fit_data, aes(x = Date)) +
  
  geom_line(aes(y = fit), color = 'black', alpha = 0.5, size = 0.5) +

  geom_point(aes(y = actual), alpha = 0.5, color = 'darkred', size = 1) +
  
  labs(x = 'Date', y = expression(y[t]),
       title = 'ARMA(1,2) Model',
       caption = 'Fitted model shown by black line and the actual values of the training set are shown in red.')

# Back-transformation the data to the original scale
dta = tsibble(train,
              fit = fit_data$fit,
              index = Date) 

dta = dta %>%
  mutate(back_fit = fit + weekly_lag + annual_lag - total_lag)

ggplot(dta, aes(x = Date)) +

  geom_line(aes(y = UniqueVisits), alpha = 0.5, color = 'black', size = 0.5) +
  
  geom_point(aes(y = UniqueVisits), alpha = 0.5, color = 'black', size = 0.5) +
  
  #geom_line(aes(y = back_fit), color = 'darkred', linetype = 'dashed', size = 0.5) +
  
  geom_point(aes(y = back_fit), color = 'darkred', size = 0.5) +
  
  labs(x = 'Date', y = 'Unique visits',
       title = 'Back-transformed ARMA(1,2) Model',
       caption = 'Fitted model shown by black line and the actual values of the training set are shown in red.')

```

## Prophet

@prophet

@fableprophet


# Results  

All the above models were trained on a training set, and the predictive accuracy was then evaluated on a test set. The training set consists of page visits starting from `r min(train$Date)` until `r max(train$Date)`, and test set contains the data from `r min(test$Date)` to `r max(test$Date)`, making up the last `r nrow(test)` observations of the data. Our primary evaluation metrics for model comparison are the root mean squared error (RMSE) and mean absolute error (MAE), which both have the advantage of being measured on the same scale as the data (i.e., the number of website visits). Using these are our primary accuracy measures gives us more interpretable results. For clarification, the RMSE and MAE are defined as  

$$ RMSE = \sqrt{ \Bigg( \frac{1}{n}\sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 \Bigg) }$$  

$$MAE=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat y_i|$$

In addition to these accuracy measures, a common-sense baseline serves as a sanity check, and is often used as a benchmark for more advanced time series models. Given daily data with yearly seasonality, a common-sense baseline is to predict the number of unique visits at time t to be equal to the number of unique visits at t-365. In other words, a random walk model making a constant prediction with yearly seasonality, which is known as a seasonal naive model (Eq. X)

$$\hat x_t = x_{t-365}$$

```{r, echo=F, message = F, warning = F, error = F}
# Plot the predictions for the transformed series
arima_forecasts = models %>%
  forecast(h = '341 days')

prophet_model = train %>%
  model(Prophet = prophet(UniqueVisits))

prophet_forecast = prophet_model %>%
  forecast(h = '341 days')

# Get the point and interval estimates for the forecast(s)
arima_data = arima_forecasts %>%
  hilo() %>%
  unpack_hilo(c(`80%`,`95%`)) %>%
  select(lower95 = `95%_lower`,
         lower80 = `80%_lower`,
         prediction = .mean,
         upper80 = `80%_upper`,
         upper95 = `95%_upper`)

prophet_data = prophet_forecast %>%
  hilo() %>%
  unpack_hilo(c(`80%`,`95%`)) %>%
  select(lower95 = `95%_lower`,
         lower80 = `80%_lower`,
         prediction = .mean,
         upper80 = `80%_upper`,
         upper95 = `95%_upper`)

arima_data = left_join(arima_data,
                       test) %>%
  # Back-transformations to get the ARIMA model results to the original scale
  mutate(back_pred = prediction + weekly_lag + annual_lag - total_lag,
         back_lower95 = lower95 + weekly_lag + annual_lag - total_lag,
         back_upper95 = upper95 + weekly_lag + annual_lag - total_lag,
         check = diff + weekly_lag + annual_lag - total_lag)

# Computes the accuracy measures for the back-transformed data
# ACCURACY MEASURES ARE THE SAME FOR BOTH TRANSFORMED AND UNTRANSFORMED TIME SERIES!!!
#arima_data %>%
#  select(.model, Date, UniqueVisits, back_pred) %>% 
#  mutate(forecast_error = UniqueVisits - back_pred,
#         percent_error = 100 * (forecast_error/UniqueVisits)) %>%
#  as_tibble() %>%
#  group_by(.model) %>%
#  summarise(ME = mean(forecast_error, na.rm = T),           # the na.rm argument will not include the training records
#           RMSE = sqrt(mean(forecast_error^2, na.rm = T)),
#           MAE = mean(abs(forecast_error), na.rm = T),
#           MPE = mean(percent_error, na.rm = T),
#           MAPE = mean(abs(percent_error), na.rm = T))
```  

```{r echo = F}
# Plot the forecasts
ggplot(data = arima_data %>% filter(.model == "Seasonal Naive"), aes(x = Date)) +
  
  geom_line(aes(y = back_pred), alpha = 0.5, linetype = 'dashed') +
  
  geom_point(aes(y = UniqueVisits), size = 1, color = 'darkred', alpha = 0.5) +

  # Add in the training data to show the historical data
  geom_line(data = train, mapping = aes(x = Date, y = UniqueVisits), size = 0.5) +
  
  coord_cartesian(xlim = c(as.Date('2019-07-14'), as.Date('2020-09-01'))) +
  
  labs(x = 'Date', y = 'Unique Visits', title = 'Seasonal Naive')
```  

```{r echo = F}
# Plot the forecasts
ggplot(data = arima_data %>% filter(.model == "ARMA(1,2)"), aes(x = Date)) +
  
  geom_line(aes(y = back_pred), alpha = 0.5, linetype = 'dashed') +
  
  geom_point(aes(y = UniqueVisits), size = 1, color = 'darkred', alpha = 0.5) +

  # Add in the training data to show the historical data
  geom_line(data = train, mapping = aes(x = Date, y = UniqueVisits), size = 0.5) +
  
  coord_cartesian(xlim = c(as.Date('2019-07-14'), as.Date('2020-09-01'))) +
  
  labs(x = 'Date', y = 'Unique Visits', title = 'ARMA(1,2)')
```  


```{r, echo = F}
# Shows various measures of accuracy in the point estimates for the forecast
accuracy(arima_forecasts, web) %>%
  add_row(accuracy(prophet_forecast, web)) %>%
  select(-.type, -ACF1) %>%
  arrange(MAE) %>%
  rename(Model = .model) %>%
  kable(digits = 2, caption = 'Accuracy measures from the test set performance of fitted models.')
```

```{r mae_plot, echo = F}
accuracy(arima_forecasts, web) %>%
  add_row(accuracy(prophet_forecast, web)) %>%
  select(.model, RMSE, MAE) %>%
  rename(Model = .model) %>%
  pivot_longer(c(MAE, RMSE), names_to = 'Metric', values_to = 'Value') %>%
  ggplot(aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(position="dodge", stat="identity") +
  labs(caption = 'Selected accuracy measures for the various fitted models.')
```

Discuss results here

# Conclusion
 \newpage  
```{r final-forecast, echo = F, fig.height = 4.5}
# Fit the final model with all the available data
final_model = web %>%
  model('ARMA(1,2)' = ARIMA(diff ~ 0 + pdq(1,0,2)))

# Select the forecast horizon in days
h = 30

# Forecast the final model
final_data = final_model %>%
  
  select('ARMA(1,2)') %>%
  
  forecast(h = paste(h, 'days')) %>%
  
  hilo() %>%
  
  unpack_hilo(c(`80%`,`95%`)) %>%
  
  select(lower95 = `95%_lower`,
         lower80 = `80%_lower`,
         diff = .mean,
         upper80 = `80%_upper`,
         upper95 = `95%_upper`) %>%
  
  add_column(UniqueVisits = NA, weekly_lag = NA, annual_lag = NA, total_lag = NA) %>%
  
  # Append the original data to the beginning of the forecast data to get lagged values
  add_row(web %>% select(UniqueVisits, diff, Date, weekly_lag, annual_lag, total_lag), .before = T) %>%
  
  # Get the lagged values to do the back-transformations
  mutate(weekly_lag = if_else(is.na(weekly_lag), lag(UniqueVisits, 7), weekly_lag),
         annual_lag = if_else(is.na(annual_lag), lag(UniqueVisits, n = 365), annual_lag),
         total_lag = if_else(is.na(total_lag), lag(lag(UniqueVisits, n = 365), n = 7), total_lag),
         back_pred = diff + weekly_lag + annual_lag - total_lag,
         back_lower95 = lower95 + weekly_lag + annual_lag - total_lag,
         back_upper95 = upper95 + weekly_lag + annual_lag - total_lag)

# Have to iteratively compute the back-transformations since we have to use some of the forecast to compute the future forecast
for ( i in (length(final_data$weekly_lag) - h):length(final_data$weekly_lag) ) {
  final_data$weekly_lag[i] = final_data$back_pred[i-7]
  final_data$annual_lag[i] = final_data$back_pred[i-365]
  final_data$total_lag[i] = final_data$back_pred[i-372]
  final_data$back_pred[i] = final_data$diff[i] + final_data$weekly_lag[i] + final_data$annual_lag[i] - final_data$total_lag[i]
  final_data$back_lower95[i] = final_data$lower95[i] + final_data$weekly_lag[i] + final_data$annual_lag[i] - final_data$total_lag[i]
  final_data$back_upper95[i] = final_data$upper95[i] + final_data$weekly_lag[i] + final_data$annual_lag[i] - final_data$total_lag[i]
}

ggplot(data = final_data, aes(x = Date)) +
  
  geom_ribbon(aes(ymin = back_lower95, ymax = back_upper95), linetype = 'dashed', alpha = 0.5, fill = 'grey', color = 'darkgrey') +
  
  geom_line(aes(y = back_pred), alpha = 0.5, linetype = 'dashed', color = 'darkred') +

  geom_point(data = final_data %>% filter(Date > max(web$Date)),
              aes(x = Date, y = back_pred), alpha = 0.5, color = 'darkred') +

  geom_line(data = web, aes(x = Date, y = UniqueVisits), size = 0.5, color = '#437cb5') +
  
  coord_cartesian(xlim = c(as.Date(max(web$Date)-10), as.Date('2020-09-18'))) +
  
  labs(x = 'Date', y = 'Unique Visits', title = paste(h,'-Day ARMA(1,2) Forecast', sep = ''))

```
```{r echo = F}
# Produce a table of the forecast
final_data %>%
  filter(Date > max(web$Date)) %>%
  select(Date, back_lower95, back_pred, back_upper95) %>%
  kable(col.names = c('Date', 'Lower 95% CI', 'Forecast','Upper 95% CI'),
        digits = 0, caption = paste(h, '-Day ARMA(1,2) Forecast.', sep = ''))
```
\newpage

# References


